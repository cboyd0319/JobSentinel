name: Performance Benchmarks

on:
  # Only run on schedule or manual trigger - benchmarks are expensive
  schedule:
    # Run benchmarks weekly on Monday at 00:00 UTC
    - cron: '0 0 * * 1'
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false  # Let benchmarks complete

defaults:
  run:
    shell: bash

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          fetch-depth: 1

      - name: Prepare Python environment
        uses: ./.github/actions/python-env
        with:
          python-version: '3.12'
          install-deps: 'true'
          cache-dependency-path: requirements.txt

      - name: Install benchmark dependencies
        run: |
          python -m pip install pytest-benchmark memory_profiler

      - name: Create benchmark script
        run: |
          cat > benchmark_jobsentinel.py << 'EOF'
          #!/usr/bin/env python3
          """
          Performance benchmarks for JobSentinel.
          
          Measures:
          - Job scraping performance
          - Filter/matcher performance
          - Database operations
          - Notification dispatch
          """
          import asyncio
          import time
          import sys
          import os
          from pathlib import Path

          # Add app src to path
          sys.path.insert(0, str(Path.cwd() / "deploy" / "common" / "app" / "src"))

          def benchmark_database():
              """Benchmark database initialization."""
              from database import init_db
              
              start = time.perf_counter()
              asyncio.run(init_db())
              elapsed = time.perf_counter() - start
              
              print(f"✓ Database init: {elapsed:.4f}s")
              return elapsed

          def benchmark_matcher():
              """Benchmark job matching logic."""
              from matchers import KeywordMatcher
              
              matcher = KeywordMatcher(['python', 'senior', 'remote'])
              test_text = "Senior Python Developer - Remote Position" * 100
              
              start = time.perf_counter()
              for _ in range(1000):
                  matcher.match(test_text)
              elapsed = time.perf_counter() - start
              
              print(f"✓ Keyword matching (1000 iterations): {elapsed:.4f}s")
              print(f"  Average: {elapsed/1000*1000:.2f}ms per match")
              return elapsed

          def main():
              print("=" * 60)
              print("JobSentinel Performance Benchmarks")
              print("=" * 60)
              print()
              
              total = 0.0
              
              try:
                  total += benchmark_database()
                  total += benchmark_matcher()
              except Exception as e:
                  print(f"⚠️  Benchmark error: {e}")
                  print("   (This is expected if dependencies are missing)")
              
              print()
              print("=" * 60)
              print(f"Total benchmark time: {total:.4f}s")
              print("=" * 60)

          if __name__ == "__main__":
              main()
          EOF

      - name: Run performance benchmarks
        run: |
          set -euo pipefail
          python benchmark_jobsentinel.py | tee benchmark-results.txt
        continue-on-error: true

      - name: Store benchmark results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: benchmark-results-${{ github.run_number }}
          path: benchmark-results.txt
          retention-days: 90
        if: always()

      - name: Add benchmark summary
        if: always()
        run: |
          {
            echo "## ⚡ Performance Benchmark Results"
            echo ""
            echo '```'
            cat benchmark-results.txt || echo "No results file found"
            echo '```'
            echo ""
            echo "_Benchmarks run weekly to track performance trends_"
            echo ""
            echo "**Run Number:** ${{ github.run_number }}"
            echo "**Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
          } >> "${GITHUB_STEP_SUMMARY}"
